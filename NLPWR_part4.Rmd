---
title: "NLP with R - Part 4: practice LDA"
author: Sonia Mazzi<br> [ONS - Data Science Campus](https://datasciencecampus.ons.gov.uk)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    fig_caption: false
    fig_width: 11
    fig_height: 6
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true 
    theme: cosmo
    df_print: paged
---

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri("pics/dsclogo.png"),
               alt = "logo", 
               style = "position: absolute; width: 180px; top:10px; right:10px; padding: 10px")
```


**Required libraries**

We will first load the libraries we will be using in what follows.

If you don't have the libraries installed you may need to execute this first

```{r eval=FALSE}
install.packages(c("readr", "dplyr", "tidyr", "stringr", "ggplot2", "kableExtra", "formattable", "gridExtra", "tidytext", "textdata", "magick", "circlize", "radarchart", "igraph", "gggraph", "widyr", "topicmodels", "tm"))
```

Now you can load the libraries

```{r message=F, warning=F}
library(readr)# read text files
library(dplyr) #data manipulation
library(tidyr)
library(stringr) #manipulate strings
library(ggplot2) #visualizations
library(ggrepel)
library(kableExtra)
library(formattable)
library(tidytext) #text mining
library(magick)
library(circlize)
library(topicmodels)
library(tm)
```

In this part of the course I will give you some indications and you will do the analysis.

**EXERCISE.** Music and Books

This is adapted from https://www.datacamp.com/community/tutorials/ML-NLP-lyric-analysis

To experience the power of topic modeling and classification, we will combine text from Prince's lyrics 
and two books:
 "Machine Learning For Dummies", and  "Why Icebergs Float", each having over 30,000 (non-distinct) words.

The data is contained in the subfolder "data" in a file called "three-sources_tidy_balanced.csv".
The following conditioning has already been applied:

* scraped the web for lyrics;
* used the `pdf_text()` function from the `pdftools` package to collect the content of the books;
* cleaned all the data, removed stop words, and created the tidy versions using the `tidytext` package;
* combined and balanced the data such that each writer (source) has the same number of words.


Songs and the pages of books are the  **documents**.


**Exercise 1.** Read in the data into an object called `three_sources_tidy`, have a glimpse at it and create the Document-Term Matrix (DTM). Store the DTM in the object `three_sources_dtm`.


In order to apply LDA for topic modelling you need to turn the tidy data into a DTM,
where each document is a row, and each column is a term. 

To do this we use the `cast_dtm()` function from the `tidytext` package.

`cast_dtm()` needs four arguments:

* `data`: the tidy tibble;
* `document`: the column that contains the documents;
* `term`: the column that contains the terms;
* `value`: the column that contains the number of times the term appears in the document.

The syntax is:

`cast_dtm(data, document, term, value)`

To cast the data into DTM format proceed as follows:

1. Read the tidy data in.
2. Count the words in each document.
3. Use `cast_dtm()` to create the DTM object `three_sources_dtm`.



Solution is below.

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

```{r message=F}
three_sources_tidy <- read_csv("data/three_sources_tidy_balanced.csv")
```


```{r message=F}
three_sources_dtm <- three_sources_tidy %>%
  count(document, word, sort = TRUE) %>% #get word count per document to pass to cast_dtm
  ungroup() %>%
  cast_dtm(document, word, n) #create a DTM with docs as rows and words as columns

three_sources_dtm
```

<br>

**Exercise 2.** Fit the LDA model with three topics. Save the results of the model fitting in an object called `three_sources_lda`.

You need to use the `LDA()` function.

Solution is below.

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

```{r}
three_sources_lda <- LDA(three_sources_dtm, k = 3, control = list(seed = 123))
```

<br>

**Exercise 3.** Explore the word-topic probabilities. Tidy the `three_sources_lda` object and extract the $\beta$ parameter for each topic and term, the probability that a certain word is generated by a certain topic. Store the results in an object called `three_sources_beta`. Using `filter()` extract the probability of the term "shake" belongs to each of the topics.

Solution is below.

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

```{r}
three_sources_beta <- tidy(three_sources_lda, matrix = "beta")

three_sources_beta %>% filter(term == "shake")
```

<br>

**Exercise 4.** Identify possible topic themes with "top words". Produce a bar plot with the top 15 words, relative to $\beta$, for each topic.

Solution is below

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

```{r}
three_sources_beta %>%
  group_by(topic) %>% 
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) %>%
  ggplot(aes(x = reorder(term, beta), y = beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  labs(x = "Term") +
  coord_flip()
```

Unsupervised learning always requires human interpretation. However, these topics clearly reflect the three sources: Topic 1 is Machine Learning, Topic 2 is Icebergs and Topic 3 is Prince.

<br>

**Exercise 5.** Classify documents: estimate the parameter $\gamma$ per document and topic, the probability that a document belongs to each topic. Store the results in an object named `three_sources_gamma`. What are the probabilities that the document "1999" belongs to each of the three topics?

Solution is below

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>


```{r}
three_sources_gamma <- tidy(three_sources_lda, matrix = "gamma") 

three_sources_gamma %>% filter(document == "purple rain")
```

The song "Purple rain" belongs to each topic, but at a different percentages, represented by the $\gamma$ value. Some documents fit in some topics better than others. As shown above, "purple rain" has an overwhelmingly higher $\gamma$ value for Topic 3.

<br>

**Exercise 6.** Produce a chord diagram that shows the percentage of documents from each source that belong to each topic

In order to create the chord diagram, join the tidy LDA model back to `three_sources_tidy where you stored the actual source (i.e. the writer), and join by document. Then if you do the proper grouping, you can get the average $\gamma$ value per source, per topic. 

<br>

<br>

<br>

<br>

<br>

```{r}
source_topic_relationship <- three_sources_gamma %>%
  #join to orig tidy data by doc to get the source field
  inner_join(three_sources_tidy, by = "document") %>%
  select(source, topic, gamma) %>%
  group_by(source, topic) %>%
  #get the avg doc gamma value per source/topic
  mutate(mean = mean(gamma)) %>%
  #remove the gamma value as you only need the mean
  select(-gamma) %>%
  ungroup() %>%
  #removing gamma created duplicates so remove them
  distinct()

source_topic_relationship
```


```{r}
#relabel topics to include the word Topic
#source_topic_relationship$topic = paste("Topic", source_topic_relationship$topic, sep = " ")
source_topic_relationship <- source_topic_relationship %>%
  mutate(topic = paste("Topic", source_topic_relationship$topic, sep = " "))
source_topic_relationship
```

```{r}
circos.clear() #very important! Reset the circular layout parameters
#assign colors to the outside bars around the circle
grid.col = c("prince" = "purple",
             "icebergs" = "blue",
             "machine_learning" = "yellow",
             "Topic 1" = "grey", "Topic 2" = "grey", "Topic 3" = "grey")

# set the global parameters for the circular layout. Specifically the gap size (15)
#this also determines that topic goes on top half and source on bottom half
circos.par(gap.after = c(rep(5, length(unique(source_topic_relationship[[1]])) - 1), 15,
                         rep(5, length(unique(source_topic_relationship[[2]])) - 1), 15))
#main function that draws the diagram. transparancy goes from 0-1
chordDiagram(source_topic_relationship, grid.col = grid.col, transparency = .2)
title("Relationship Between Topic and Source")
```

Even though the documents from each source appear in each topic, each source clearly has a majority in a specific topic. For example, Prince songs are mostly associated with Topic 3 as that chord is larger than the chords going to topics one and two. The same thing occurs for Icebergs and Topic 1, and Machine Learning and Topic 2. Once you see the pattern here, it becomes very clear what is happening.

If you think about what you have just done, you have actually classified documents into Topics based on the mean of $\gamma$ for a topic/source. If you know the writer of a document, and you know the writer composes most lyrics or books on a specific topic, you might assume that artists of similar genres would fall into the same topics and therefore you can recommend writers that are similar.

**Exercise 7.** Top (relative to $\gamma$) Documents Per Topic

Now look at the individual document level and view the top documents per topic.

<br>

<br>

<br>

<br>

<br>

<br>

<br>

<br>


```{r}
k <- 3 #nr of topics
number_of_documents <- 5 #number of top docs to view
title <- paste("LDA Top Documents for", k, "Topics")

#same process as used with the top words
top_documents <- three_sources_gamma %>%
  group_by(topic) %>%
  arrange(topic, desc(gamma)) %>%
  slice(seq_len(number_of_documents)) %>%
  arrange(topic, gamma) %>%
  mutate(row = row_number()) %>%
  ungroup() %>%
  #re-label topics
  mutate(topic = paste("Topic", topic, sep = " ")) %>% 
  select(-gamma) %>%
  pivot_wider(names_from = topic, values_from = document) %>%
  select(-row)

top_documents
```

**Exercise 8.** The `LDA()` function accepts two methods for fitting the LDA model: "VEM" and "Gibbs". The default is "VEM". 

Try the same analysis with `LDA(three_sources_dtm, k=3, method = "Gibbs", control = list(seed = 123))`. 

Which method do you think works best?

<br>

<br>

<br>

<br>

```{r}
three_sources_lda_gibbs <- LDA(three_sources_dtm, k = 3, method = "Gibbs", control = list(seed = 123))
```


```{r}
three_sources_beta_gibbs <- tidy(three_sources_lda_gibbs, matrix = "beta")

three_sources_beta_gibbs %>% filter(term == "shake")
```



```{r}
three_sources_beta_gibbs %>%
  group_by(topic) %>% 
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta)) %>%
  ggplot(aes(x = reorder(term, beta), y = beta, fill = topic)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic, scales = "free") +
  labs(x = "Term") +
  coord_flip()
```


```{r}
three_sources_gamma_gibbs <- tidy(three_sources_lda_gibbs, matrix = "gamma") 

three_sources_gamma_gibbs %>% filter(document == "purple rain")
```


```{r}
source_topic_relationship <- three_sources_gamma_gibbs %>%
  #join to orig tidy data by doc to get the source field
  inner_join(three_sources_tidy, by = "document") %>%
  select(source, topic, gamma) %>%
  group_by(source, topic) %>%
  #get the avg doc gamma value per source/topic
  mutate(mean = mean(gamma)) %>%
  #remove the gamma value as you only need the mean
  select(-gamma) %>%
  ungroup() %>%
  #removing gamma created duplicates so remove them
  distinct()

source_topic_relationship
```


```{r}
#relabel topics to include the word Topic
#source_topic_relationship$topic = paste("Topic", source_topic_relationship$topic, sep = " ")
source_topic_relationship <- source_topic_relationship %>%
  mutate(topic = paste("Topic", source_topic_relationship$topic, sep = " "))
source_topic_relationship
```

```{r}
circos.clear() #very important! Reset the circular layout parameters
#assign colors to the outside bars around the circle
grid.col = c("prince" = "purple",
             "icebergs" = "blue",
             "machine_learning" = "yellow",
             "Topic 1" = "grey", "Topic 2" = "grey", "Topic 3" = "grey")

# set the global parameters for the circular layout. Specifically the gap size (15)
#this also determines that topic goes on top half and source on bottom half
circos.par(gap.after = c(rep(5, length(unique(source_topic_relationship[[1]])) - 1), 15,
                         rep(5, length(unique(source_topic_relationship[[2]])) - 1), 15))
#main function that draws the diagram. transparancy goes from 0-1
chordDiagram(source_topic_relationship, grid.col = grid.col, transparency = .2)
title("Relationship Between Topic and Source")
```


```{r}
k <- 3 #nr of topics
number_of_documents <- 5 #number of top docs to view
title <- paste("LDA Top Documents for", k, "Topics")

#same process as used with the top words
top_documents <- three_sources_gamma_gibbs %>%
  group_by(topic) %>%
  arrange(topic, desc(gamma)) %>%
  slice(seq_len(number_of_documents)) %>%
  arrange(topic, gamma) %>%
  mutate(row = row_number()) %>%
  ungroup() %>%
  #re-label topics
  mutate(topic = paste("Topic", topic, sep = " ")) %>% 
  select(-gamma) %>%
  pivot_wider(names_from = topic, values_from = document) %>%
  select(-row)

top_documents
```







